{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. dev data to e2e eval data  \n",
    "qa form으로 만들기 위해서 답이 될 수 있는 output의 set을 list로 만들어야 한다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_data(input_file_questions, input_file_answers, output_file, output_format='{input}\\t{output_list}'):\n",
    "    with open(input_file_questions, 'r') as f:\n",
    "        questions = f.readlines()\n",
    "\n",
    "    with open(input_file_answers, 'r') as f:\n",
    "        answers = f.readlines()\n",
    "\n",
    "    with open(output_file, 'w') as f:\n",
    "        for q, a in zip(questions, answers):\n",
    "            question = q.strip()\n",
    "            answer_list = a.strip().split('\\t')\n",
    "            output_line = output_format.format(input=question, output_list=str(answer_list))\n",
    "            f.write(output_line + '\\n')\n",
    "\n",
    "input_file_questions = \"original/questions.txt\"\n",
    "input_file_answers = \"original/answers.txt\"\n",
    "output_file = \"gold_data.txt\"\n",
    "combine_data(input_file_questions, input_file_answers, output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Split Data  \n",
    "데이터를 train, validation, test set으로 나누어야 한다.  \n",
    "이때, e2e와 retrieval evaluation을 편하게 하기 위해 문서를 많이 나누어야 한다.  \n",
    "\n",
    "file1 = \"original/biencoder-nq-dev.pages\"\n",
    "file2 = \"original/biencoder-nq-dev.questions\"\n",
    "file3 = \"gold_data.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def split_data(question_file, answer_file, gold_data_file, train_ratio=0.5, val_ratio=0.25, test_ratio=0.25):\n",
    "    with open(question_file, 'r') as qf, open(answer_file, 'r') as af, open(gold_data_file, 'r') as gf:\n",
    "        questions = qf.readlines()\n",
    "        answers = af.readlines()\n",
    "        gold_datas = gf.readlines()\n",
    "\n",
    "    data = list(zip(questions, answers, gold_datas))\n",
    "    random.shuffle(data)\n",
    "\n",
    "    train_size = int(len(data) * train_ratio)\n",
    "    val_size = int(len(data) * val_ratio)\n",
    "    test_size = len(data) - train_size - val_size\n",
    "\n",
    "    train_data = data[:train_size]\n",
    "    val_data = data[train_size:train_size + val_size]\n",
    "    test_data = data[train_size + val_size:]\n",
    "\n",
    "    return train_data, val_data, test_data\n",
    "\n",
    "def save_data(data, prefix):\n",
    "    with open(f\"{prefix}_question.txt\", 'w') as qf, open(f\"{prefix}_answer.txt\", 'w') as af, open(f\"{prefix}_retrieval.txt\", 'w') as gf:\n",
    "        for q, a, g in data:\n",
    "            qf.write(q)\n",
    "            af.write(a)\n",
    "            gf.write(g)\n",
    "\n",
    "\n",
    "question_file = \"ext-source\"  \n",
    "answer_file = \"ext-target\"\n",
    "gold_data_file = \"ext-retrieval\"\n",
    "\n",
    "try:\n",
    "    train_data, val_data, test_data = split_data(question_file, answer_file, gold_data_file)\n",
    "\n",
    "    save_data(train_data, \"train\")\n",
    "    save_data(val_data, \"val\")\n",
    "    save_data(test_data, \"test\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File not found. Please check the file paths.\")\n",
    "except PermissionError:\n",
    "    print(f\"Error: Permission denied. Please check file permissions.\")\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "save_data(train_data, \"train\")\n",
    "save_data(val_data, \"val\")\n",
    "save_data(test_data, \"test\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## temp: list로 변환  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_data(input_file_answers, output_file, output_format='{output_list}'):\n",
    "    with open(input_file_answers, 'r') as f:\n",
    "        answers = f.readlines()\n",
    "\n",
    "    with open(output_file, 'w') as f:\n",
    "        for a in answers:\n",
    "            answer_list = a.strip().split('\\t')\n",
    "            output_line = output_format.format(output_list=str(answer_list))\n",
    "            f.write(output_line + '\\n')\n",
    "\n",
    "input_file_answers = \"./train_answer.txt\"\n",
    "output_file = \"train.target\"\n",
    "combine_data(input_file_answers, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 데이터 무작위 추출  \n",
    "데이터를 무작위로 추출하는 코드  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def sample_data(files, num_samples=1000):\n",
    "    \"\"\"\n",
    "    여러 파일에서 무작위로 샘플링된 데이터를 추출합니다.\n",
    "\n",
    "    Args:\n",
    "        files: 파일 경로 리스트\n",
    "        num_samples: 추출할 샘플 수 (기본값: 200)\n",
    "\n",
    "    Returns:\n",
    "        list: 샘플링된 데이터 (각 요소는 파일별 데이터 리스트)\n",
    "    \"\"\"\n",
    "    with open(files[0], \"r\", encoding=\"utf-8\") as f0, \\\n",
    "         open(files[1], \"r\", encoding=\"utf-8\") as f1, \\\n",
    "         open(files[2], \"r\", encoding=\"utf-8\") as f2:\n",
    "\n",
    "        lines = list(zip(f0, f1, f2))\n",
    "        sampled_lines = random.sample(lines, num_samples)\n",
    "\n",
    "        sampled_data = [\n",
    "            [line[i].strip() for line in sampled_lines]\n",
    "            for i in range(len(files))\n",
    "        ]\n",
    "\n",
    "    return sampled_data\n",
    "\n",
    "def save_sampled_data(sampled_data, output_files):\n",
    "    \"\"\"\n",
    "    샘플링된 데이터를 각각의 출력 파일에 저장합니다.\n",
    "\n",
    "    Args:\n",
    "        sampled_data: 샘플링된 데이터\n",
    "        output_files: 출력 파일 경로 리스트\n",
    "    \"\"\"\n",
    "    for i, data in enumerate(sampled_data):\n",
    "        with open(output_files[i], \"w\", encoding=\"utf-8\") as f:\n",
    "            for line in data:\n",
    "                f.write(f\"{line}\\n\")\n",
    "\n",
    "# 파일 경로 설정\n",
    "input_files = [\"source\", \"target\", \"retrieval\"]\n",
    "output_files = [\"ext-source\", \"ext-target\", \"ext-retrieval\"]\n",
    "\n",
    "# 데이터 샘플링 및 저장\n",
    "sampled_data = sample_data(input_files)\n",
    "save_sampled_data(sampled_data, output_files)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
