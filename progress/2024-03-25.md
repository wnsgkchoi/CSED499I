---
title:  "3월 25일 연구 진행 상황" 

categories:
  - Conformal_RAG
tags:
  - [RAG, Conformal_Prediction, Research]

toc: true
toc_sticky: true

date: 2024-03-25
last_modified_at: 2024-03-25
---  

> 긴 말 필요 없이 바로 시작한다. 시간이 없다..  

## 0) 오류 찾기  
분명 내 로컬 컴퓨터의 WSL에서는 아래와 같이 실행하면 evaluation이 정상적으로 실행되었다.  

```
    readme.md의 retrieval evaluation 부분을 읽고 해당 부분을 그대로 따라하여 검색된 문서의 정확도를 측정해보았다.  

    먼저 아래 코드를 실행했다.  
    ```bash
    wget https://dl.fbaipublicfiles.com/dpr/data/retriever/biencoder-nq-dev.json.gz && gzip -d biencoder-nq-dev.json.gz  
    ```
    그런데 wget에서 문제가 생겨 해당 링크에 바로 접속해 파일을 다운받고, 이를 wsl에 옮긴 뒤 gzip해주었다.  

    다음으로 아래 코드를 실행했다.  

    ```bash
    python examples/research_projects/rag/parse_dpr_relevance_data.py \
        --src_path biencoder-nq-dev.json \
        --evaluation_set output/biencoder-nq-dev.questions \
        --gold_data_path output/biencoder-nq-dev.pages
    ```

    마지막으로 아래 코드를 실행했다.  

    ```bash
    python examples/research_projects/rag/eval_rag.py \
        --model_name_or_path facebook/rag-sequence-base \
        --model_type rag_sequence \
        --evaluation_set output/biencoder-nq-dev.questions \
        --gold_data_path output/biencoder-nq-dev.pages \
        --predictions_path output/retrieval_preds.tsv  \
        --eval_mode retrieval \
        --k 1
    ```

    일단 결과는 위와 같이 나왔다.  
    점수 68.80이다.  
```

위의 코드로 돌리니까 갑자기 evaluation이 정상적으로 돌아간다.  
model을 rag-sequence-nq에서 rag-sequence-base로 바꾸었더니 갑자기 된다.  
아마 결과는 이전과 똑같이 68.80이 나올 것으로 예상하지만, 일단 코드를 다시 돌려본다.  

오류를 찾았다기 보다는.. 뭐 아마 rag-sequence-nq와 관련해 어떤 데이터의 주소를 잘못 넘겼나보다.  
아무튼 evaluation의 결과는 아래와 같이 나왔고, 과정이 어찌 되었든, 오류는 해결했다.  
```bash
Some weights of the model checkpoint at facebook/rag-sequence-base were not used when initializing RagSequenceForGeneration: ['rag.question_encoder.question_encoder.bert_model.pooler.dense.bias', 'rag.question_encoder.question_encoder.bert_model.pooler.dense.weight']
- This IS expected if you are initializing RagSequenceForGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RagSequenceForGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
All the weights of RagSequenceForGeneration were initialized from the model checkpoint at facebook/rag-sequence-base.
If your task is similar to the task the model of the checkpoint was trained on, you can already use RagSequenceForGeneration for predictions without further training.
Generation config file not found, using a generation config created from the model config.
initializing retrieval
Loading index from wiki_dpr with index name exact
Loading dataset shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 161/161 [00:14<00:00, 11.45it/s]
0it [00:00, ?it/s]We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.
6515it [01:16, 84.72it/s]
INFO:__main__:Precision@1:  68.80
```
위의 evaluation은 retrieval evaluation으로 질문에 관한 문서 검색 능력을 평가하는 것이다. 사실 이번 연구는 retriever의 성능을 개선하는 연구이기 때문에 이 evaluation만 사용해도 괜찮겠지만, 일단 e2e evaluation도 실행해본다. readme.md에 있는 설명을 정리하면 아래와 같다.  
```
먼저 gold_data로 두 가지의 format을 지원한다.  
첫 번째는 `qa` -> `input [tab] output_list` 형식의 format이다. 예를 들면 who is the owner of reading football club	['Xiu Li Dai', 'Dai Yongge', 'Dai Xiuli', 'Yongge Dai']  
두 번째는 `ans` -> 하나의 expected answer만을 포함하는 format이다. 예를 들면 Xiu Li Dai  

아래와 같은 형식의 명령어로 실행 가능하다.
  python examples/research_projects/rag/eval_rag.py \
    --model_name_or_path facebook/rag-sequence-nq \
    --model_type rag_sequence \
    --evaluation_set path/to/test.source \
    --gold_data_path path/to/gold_data \
    --predictions_path path/to/e2e_preds.txt \
    --eval_mode e2e \
    --gold_data_mode qa \
    --n_docs 5 \ # You can experiment with retrieving different number of documents at evaluation time
    --print_predictions \
    --recalculate \ # adding this parameter will force recalculating predictions even if predictions_path already exists
```

아래와 같이 코드를 돌려보았다.  
```bash
python examples/research_projects/rag/eval_rag.py \
    --model_name_or_path facebook/rag-sequence-base \
    --model_type rag_sequence \
    --evaluation_set output/biencoder-nq-dev.questions \
    --gold_data_path output/biencoder-nq-dev.pages \
    --predictions_path output/retrieval_preds.tsv \
    --eval_mode e2e \
    --gold_data_mode qa \
    --n_docs 5 \
    --print_predictions
```
위의 코드는 문제가 있었고, 아직 해결하지 못했다. gold_data_path가 csv로 주어져야 하는데, 그렇지 않기 때문이다. 위에서 수행한 retrieval evaluation의 gold_data_path를 그대로 쓰면 될 줄 알았지만, 그렇지 않았다.  

## 1) top-k의 k에 따른 score 경향 파악  
저번 미팅에서 top-k의 k를 변화시키며 경향을 파악하기로 했는데, k를 어디에서 변화시키는지 아직 감이 안 잡힌다.  
내가 공부한 것이 맞다면, RAG의 top-k의 k는 parameter이고, k가 정해진 상태라면 MIPS를 사용해 k개를 retrieve한다. 그래서 k를 정하는 위치가 MIPS를 하는 곳 근처에 있을 것으로 예상하지만, MIPS 코드가 어디에 있는지 잘 모르겠다. MIPS가 retriever 안에서 처리되므로 distributed_pytorch_retriever.py에서 찾아보았는데 잘 모르겠다...  



[맨 위로 이동하기](#){: .btn .btn--primary }{: .align-right}