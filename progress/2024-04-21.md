---
title:  "4월 21일 연구 진행 상황" 

categories:
  - Conformal_RAG
tags:
  - [RAG, Conformal_Prediction, Research]

toc: true
toc_sticky: true

date: 2024-04-21
last_modified_at: 2024-04-24
---  

## 1. MIPS 코드 찾기  
저번 미팅까지 진행했던 부분 이후부터 진행한다.  

evaluation 코드를 확실하게 분석하여 MIPS 부분을 찾으려 한다.  
```python
  def main(args):
    model_kwargs = {}
    if args.model_type is None:
        args.model_type = infer_model_type(args.model_name_or_path)
        assert args.model_type is not None
    if args.model_type.startswith("rag"):
        model_class = RagTokenForGeneration if args.model_type == "rag_token" else RagSequenceForGeneration
        model_kwargs["n_docs"] = args.n_docs
        if args.index_name is not None:
            model_kwargs["index_name"] = args.index_name
        if args.index_path is not None:
            model_kwargs["index_path"] = args.index_path
    else:
        model_class = BartForConditionalGeneration

    checkpoints = (
        [f.path for f in os.scandir(args.model_name_or_path) if f.is_dir()]
        if args.eval_all_checkpoints
        else [args.model_name_or_path]
    )

    logger.info("Evaluate the following checkpoints: %s", checkpoints)

    score_fn = get_scores if args.eval_mode == "e2e" else get_precision_at_k
    evaluate_batch_fn = evaluate_batch_e2e if args.eval_mode == "e2e" else evaluate_batch_retrieval

    for checkpoint in checkpoints:
        if os.path.exists(args.predictions_path) and (not args.recalculate):
            logger.info("Calculating metrics based on an existing predictions file: {}".format(args.predictions_path))
            score_fn(args, args.predictions_path, args.gold_data_path)
            continue

        logger.info("***** Running evaluation for {} *****".format(checkpoint))
        logger.info("  Batch size = %d", args.eval_batch_size)
        logger.info("  Predictions will be stored under {}".format(args.predictions_path))

        if args.model_type.startswith("rag"):
            retriever = RagRetriever.from_pretrained(checkpoint, **model_kwargs)
            model = model_class.from_pretrained(checkpoint, retriever=retriever, **model_kwargs)
            model.retriever.init_retrieval()
        else:
            model = model_class.from_pretrained(checkpoint, **model_kwargs)
        model.to(args.device)

        with open(args.evaluation_set, "r") as eval_file, open(args.predictions_path, "w") as preds_file:
            questions = []
            for line in tqdm(eval_file):
                questions.append(line.strip())
                if len(questions) == args.eval_batch_size:
                    answers = evaluate_batch_fn(args, model, questions)
                    preds_file.write("\n".join(answers) + "\n")
                    preds_file.flush()
                    questions = []
            if len(questions) > 0:
                answers = evaluate_batch_fn(args, model, questions)
                preds_file.write("\n".join(answers))
                preds_file.flush()

            score_fn(args, args.predictions_path, args.gold_data_path)
```  
위의 코드는 eval_rag.py의 main 함수의 코드이다. 맨 위부터 천천히 뜯어본다.  

```python  
  model_kwargs = {}
    if args.model_type is None:
        args.model_type = infer_model_type(args.model_name_or_path)
        assert args.model_type is not None
    if args.model_type.startswith("rag"):
        model_class = RagTokenForGeneration if args.model_type == "rag_token" else RagSequenceForGeneration
        model_kwargs["n_docs"] = args.n_docs
        if args.index_name is not None:
            model_kwargs["index_name"] = args.index_name
        if args.index_path is not None:
            model_kwargs["index_path"] = args.index_path
    else:
        model_class = BartForConditionalGeneration
```
먼저, model_kwargs를 선언한다. 그 뒤에, 터미널에 입력한 인자 중에서 model_type을 기반으로 model_class를 결정한다. 만약 model_type이 rag의 일종이라면, model_kwargs의 "n_docs"에 n_docs를 넣는다. model이 rag가 아니라면 BART를 사용하는데, 본 연구에서는 RAG를 다루기 때문에 사용되지 않는다.  

```python
    checkpoints = (
        [f.path for f in os.scandir(args.model_name_or_path) if f.is_dir()]
        if args.eval_all_checkpoints
        else [args.model_name_or_path]
    )

    logger.info("Evaluate the following checkpoints: %s", checkpoints)
```
checkpoint는 말 그대로 checkpoint다.  

```python
    score_fn = get_scores if args.eval_mode == "e2e" else get_precision_at_k
    evaluate_batch_fn = evaluate_batch_e2e if args.eval_mode == "e2e" else evaluate_batch_retrieval

```
evaluation할 때, e2e가 아닌 retrieval로 하므로, score_fn은 get_precision_at_k, evaluation_batch_fn은 evaluate_batch_retrieval로 결정된다. 각각은 함수로 구현되어 있다. 이는 아래에서 살펴볼 예정이다.  

```python
    for checkpoint in checkpoints:
        if os.path.exists(args.predictions_path) and (not args.recalculate):
            logger.info("Calculating metrics based on an existing predictions file: {}".format(args.predictions_path))
            score_fn(args, args.predictions_path, args.gold_data_path)
            continue

        logger.info("***** Running evaluation for {} *****".format(checkpoint))
        logger.info("  Batch size = %d", args.eval_batch_size)
        logger.info("  Predictions will be stored under {}".format(args.predictions_path))

        if args.model_type.startswith("rag"):
            retriever = RagRetriever.from_pretrained(checkpoint, **model_kwargs)
            model = model_class.from_pretrained(checkpoint, retriever=retriever, **model_kwargs)
            model.retriever.init_retrieval()
        else:
            model = model_class.from_pretrained(checkpoint, **model_kwargs)
        model.to(args.device)

        with open(args.evaluation_set, "r") as eval_file, open(args.predictions_path, "w") as preds_file:
            questions = []
            for line in tqdm(eval_file):
                questions.append(line.strip())
                if len(questions) == args.eval_batch_size:
                    answers = evaluate_batch_fn(args, model, questions)
                    preds_file.write("\n".join(answers) + "\n")
                    preds_file.flush()
                    questions = []
            if len(questions) > 0:
                answers = evaluate_batch_fn(args, model, questions)
                preds_file.write("\n".join(answers))
                preds_file.flush()

            score_fn(args, args.predictions_path, args.gold_data_path)
```
먼저 checkpoints의 각각의 checkpoint마다 다음을 수행한다. 이때, predictions_path가 존재하면, 해당 데이터를 바탕으로 바로 계산을 수행하고, score_fn을 계산한 뒤 evaluation을 종료한다.  

만약 predictions_path가 없다면, prediction을 생성해야 한다. 각각의 question에 대해 대답을 생성하고, predictions_path에 이를 쓴다.  

이때 answer를 생성하는 과정이 evaluate_batch_fn 함수로 이루어지는 것을 볼 수 있다. 그리고 이 함수는 위에서 evaluate_batch_retrieval로 설정되었을 것이다. 이제 위에서 추가로 살펴보기로 한 두 함수를 더 살펴보자.  

- get_precision_at_k  
```python  
def get_precision_at_k(args, preds_path, gold_data_path):
    k = args.k
    hypos = [line.strip() for line in open(preds_path, "r").readlines()]
    references = [line.strip() for line in open(gold_data_path, "r").readlines()]

    em = total = 0
    for hypo, reference in zip(hypos, references):
        hypo_provenance = set(hypo.split("\t")[:k])
        ref_provenance = set(reference.split("\t"))
        total += 1
        em += len(hypo_provenance & ref_provenance) / k

    em = 100.0 * em / total
    logger.info(f"Precision@{k}: {em: .2f}")
```
이 코드는 이미 분석했다. 2024-04-15.md를 참조하자.  

- evaluate_batch_retrieval  
```python
def evaluate_batch_retrieval(args, rag_model, questions):
    def strip_title(title):
        if title.startswith('"'):
            title = title[1:]
        if title.endswith('"'):
            title = title[:-1]
        return title

    retriever_input_ids = rag_model.retriever.question_encoder_tokenizer.batch_encode_plus(
        questions,
        return_tensors="pt",
        padding=True,
        truncation=True,
    )["input_ids"].to(args.device)

    question_enc_outputs = rag_model.rag.question_encoder(retriever_input_ids)
    question_enc_pool_output = question_enc_outputs[0]

    result = rag_model.retriever(
        retriever_input_ids,
        question_enc_pool_output.cpu().detach().to(torch.float32).numpy(),
        prefix=rag_model.rag.generator.config.prefix,
        n_docs=rag_model.config.n_docs,
        return_tensors="pt",
    )
    all_docs = rag_model.retriever.index.get_doc_dicts(result.doc_ids)
    provenance_strings = []
    for docs in all_docs:
        provenance = [strip_title(title) for title in docs["title"]]
        provenance_strings.append("\t".join(provenance))
    return provenance_strings
```
이 함수의 동작을 크게 4가지의 과정으로 나눌 수 있다.  
1. Question 전처리  
    먼저 strip_title을 정의하여 title의 " 표시를 제거한다.  
    그 뒤에 있는 부분에서 tokenizer를 통해 question을 encoder가 이해할 수 있는 numerical한 표현으로 수정한다.  
    그 뒤에 이를 args.device에 이동시킨다.  

2. question encoding  
    터미널로 넘긴 rag_model의 encoder에 전처리한 question을 넣고, encoding한 결과가 question_enc_outputs에 저장된다. 그리고, 이 중에서 첫 번째 element만 따로 question_enc_pool_output에 저장된다.  

3. Document Retrieving  
    rag_model의 retriever에 encoding된 question batch와 여러 인자를 넘겨 result를 생성한다.  

4. Retrieved Documents Processing  
    all_docs에는 retriever가 retrieve한 모든 document가 저장된다. provenance_string에는 retrieve된 document마다 가지고 있는 title들을 전처리한 뒤, title마다 tab으로 구분하여 join한 뒤 append한다.  
    마지막으로 이렇게 나온 provenance_string을 return한다.  

MIPS를 사용하는 부분은, result를 생성하는 rag_model.retriever에 있어야 하는데, 이 retriever에 들어가는 친구는 이 코드에서 확인 가능하다.  
```python
...
    if args.model_type.startswith("rag"):
        retriever = RagRetriever.from_pretrained(checkpoint, **model_kwargs)
        model = model_class.from_pretrained(checkpoint, retriever=retriever, **model_kwargs)
        model.retriever.init_retrieval()
...
```
즉, retriever는 from_pretrained에서 얻어지며, 이 from_pretrained에 넘겨지는 인자들 중 하나인 checkpoint는 terminal에서 명령어로 넘기는 인자인 model_name_or_path에서 설정한다. <span style="color:red"> **문제점이 있는데, README.md에는 transformer에서 기본적으로 제공하는 facebook/rag-sequence-base와 facebook/rag-token-base를 기반으로 finetuning은 가능하지만, 처음부터 학습시키는 방법에 대한 내용은 없다.** </span> 이 부분을 어떻게 할지 교수님과 상의를 해봐야겠다. 물론 상의 이전에 처음부터 학습시키는 것이 가능한지, 가능하다면 어떻게 할 수 있는지를 알아보고 교수님과 상의해볼 예정이다. (사실 이게 불가능하면 연구가 막히긴 함.)  


## 2. k관련 오류 찾기  
4월 19일에 진행했던 미팅에서 다음을 수행해보기로 했다.  
1. 먼저 true data의 스코어  
2. k 5 이상일 때 변하지 않는 이유 찾기  
3. 히스토그램 그리기 (녹음 참고) -> 정답이 set이니까 (여러 개) 각각의 정답에 대해 MIPS를 적용하여 max, min, median, mean 등을 나타낼 수 있을 것.  

이 중에서 가장 중요한 것은 아마도 2번일 것이다. 왜냐하면, k=5는 굉장히 작은 숫자인데, 이 숫자부터 score가 똑같기 때문이다. 그래서 2번부터 진행할 것이다.  

먼저, eval_rag.py의 get_precision_at_k에서 hypo_provenance를 출력해보자. 그리고, hypo_provenance set의 원소 개소의 최대가 몇인지도 출력해보자. 아래와 같이 코드를 수정했다.  

```python
#eval_rag.py  
def get_precision_at_k(args, preds_path, gold_data_path):
    k = args.k
    hypos = [line.strip() for line in open(preds_path, "r").readlines()]
    references = [line.strip() for line in open(gold_data_path, "r").readlines()]
    file = open("hypo_at_k.txt", 'w')
    for hypo in hypos:
        file.write(hypo + "\n\n")

    em = total = 0
    max_hypo = 0
    for hypo, reference in zip(hypos, references):
        hypo_provenance = set(hypo.split("\t")[:k])
        ref_provenance = set(reference.split("\t"))
        max_hypo = max(max_hypo, len(hypo_provenance))
        
        total += 1
        #em += len(hypo_provenance & ref_provenance) / k
        em += 1 if len(hypo_provenance & ref_provenance) else 0
   
    em = 100.0 * em / total
    logger.info(f"Precision@{k}: {em: .2f}")
    logger.info(f"max_hypo@{k}: {max_hypo: .1f}")
```
결과는 예상한 대로 hypo가 5개까지만 나오고, 그 이상은 나오지 않는다. 이제 5개 이상 나오지 않는 이유를 분석해야 한다. 일단 hypos가 preds_path의 line을 나눈 것이므로, preds_path가 어떻게 생성되는지 파악해야 한다.  

preds_path를 작성하는 코드는 위에서 분석했던 main 함수의 이 부분이다.  

```python
        with open(args.evaluation_set, "r") as eval_file, open(args.predictions_path, "w") as preds_file:
            questions = []
            for line in tqdm(eval_file):
                questions.append(line.strip())
                if len(questions) == args.eval_batch_size:
                    answers = evaluate_batch_fn(args, model, questions)
                    preds_file.write("\n".join(answers) + "\n")
                    preds_file.flush()
                    questions = []
            if len(questions) > 0:
                answers = evaluate_batch_fn(args, model, questions)
                preds_file.write("\n".join(answers))
                preds_file.flush()

            score_fn(args, args.predictions_path, args.gold_data_path)
```
score_fn = get_predcision_at_k임. eval_file을 tqdm으로 encoding하여 line마다 questions에 넣어준다. 그리고 evaluate_batch_fn 함수로 answer를 생성했었다.  

원인을 찾았다. 이유는 바로 rag_model에 기본적으로 내장된 config.n_docs 때문이다. 이 숫자를 terminal의 인자로 넘겨줄 수 있는데, readme.md에서 이를 발견하지 못해서 몰랐다.  

바로 추가해서 코드를 돌려본다. 돌릴 코드는 아래와 같다.  
```bash
python examples/research_projects/rag/eval_rag.py \
--model_name_or_path facebook/rag-sequence-base \
--model_type rag_sequence \
--evaluation_set output/biencoder-nq-dev.questions \
--gold_data_path output/biencoder-nq-dev.pages \
--predictions_path output/retrieval_preds.tsv \
--eval_mode retrieval \
--k 1000 \
--recalculate \
--n_docs 1000
```

다시 결과를 정리해보았다.  
|k|score|  
|---|---|  
|1|68.80|  
|2|75.84|  
|3|79.26|  
|4|81.40|  
|5|82.62|  
|7|84.30|  
|10|85.74|  
|49|89.95|  
|50|90.01|  
|100|91.31|  
|500|93.05|  
|1000|93.51|  

처음 이 과제를 진행할 때, score가 90이 되는 부분을 기준으로 삼기로 했었는데, k가 50 이상일 때 score가 90 이상이 됨을 확인할 수 있다. 이렇게 k 관련 오류를 해결할 수 있었다.  


## 3. Retriever 수정하기  
Retriever를 수정하기 위해서는 facebook/rag-sequence-base의 실제 코드를 얻을 필요가 있었고, 이 코드는 다행히도 LLM 모델을 수집하고 제공하는 huggingface에서 얻을 수 있었다. 
<span style="color:red"> **문제가 발생했다. pytorch_model.bin을 풀어야 되는데, 할 수가 없는 상황이다..** </span>  
일단 windows 환경에서 압축을 해제하고, 이를 서버에 옮기는 방법을 써보려고 한다. 일단 옮겼는데, 이게 코드로 배포되지는 않는가보다.. 이건 날이 밝으면 교수님께 여쭤봐야겠다.  

일어나면 할 것.  
1. 교수님께 retriever 코드를 어떻게 얻을 수 있는지 여쭤보기 (이게 안 되면 MIPS 부분을 찾을 수가 없어요..) 
2. 먼저 true data의 스코어  
3. 히스토그램 그리기 (녹음 참고) -> 정답이 set이니까 (여러 개) 각각의 정답에 대해 MIPS를 적용하여 max, min, median, mean 등을 나타낼 수 있을 것.  

교수님께 여쭤보니, retriever 코드가 없는 것은 말이 안 된다고 한다. 이전에 찾았던 distributed_pytorch_retriever.py의 class를 사용하는 것이 맞는 듯하다. 문제는 이 코드를 어디에서 불러오는지 모르겠다.  

```python
    def retrieve(self, question_hidden_states: np.ndarray, n_docs: int) -> Tuple[np.ndarray, List[dict]]:
        """
        Retrieves documents for specified ``question_hidden_states``. The main process, which has the access to the index stored in memory, gathers queries
        from all the processes in the main training process group, performs the retrieval and scatters back the results.

        Args:
            question_hidden_states (:obj:`np.ndarray` of shape :obj:`(batch_size, vector_size)`):
                A batch of query vectors to retrieve with.
            n_docs (:obj:`int`):
                The number of docs retrieved per query.

        Output:
            retrieved_doc_embeds (:obj:`np.ndarray` of shape :obj:`(batch_size, n_docs, dim)`
                The retrieval embeddings of the retrieved docs per query.
            doc_ids (:obj:`np.ndarray` of shape :obj:`batch_size, n_docs`)
                The ids of the documents in the index
            doc_dicts (:obj:`List[dict]`):
                The retrieved_doc_embeds examples per query.
        """

        # single GPU training
        if not dist.is_initialized():
            doc_ids, retrieved_doc_embeds = self._main_retrieve(question_hidden_states, n_docs)
            return retrieved_doc_embeds, doc_ids, self.index.get_doc_dicts(doc_ids)

        # distributed training
        world_size = dist.get_world_size(group=self.process_group)

        # gather logic
        gather_list = None
        if self._is_main():
            gather_list = [torch.empty(question_hidden_states.shape, dtype=torch.float32) for _ in range(world_size)]
        dist.gather(torch.tensor(question_hidden_states), dst=0, gather_list=gather_list, group=self.process_group)

        # scatter logic
        n_queries = question_hidden_states.shape[0]
        scatter_ids = []
        scatter_vectors = []
        if self._is_main():
            assert len(gather_list) == world_size
            ids, vectors = self._main_retrieve(torch.cat(gather_list).numpy(), n_docs)
            ids, vectors = torch.tensor(ids), torch.tensor(vectors)
            scatter_ids = self._chunk_tensor(ids, n_queries)
            scatter_vectors = self._chunk_tensor(vectors, n_queries)
        doc_ids = self._scattered(scatter_ids, [n_queries, n_docs], target_type=torch.int64)
        retrieved_doc_embeds = self._scattered(scatter_vectors, [n_queries, n_docs, question_hidden_states.shape[1]])

        return retrieved_doc_embeds.numpy(), doc_ids.numpy(), self.index.get_doc_dicts(doc_ids)
```
일단 이 코드가 retrieve하는 코드다.  

python Original_RAG/eval_rag.py --model_name_or_path facebook/rag-sequence-base --model_type rag_sequence --evaluation_set Original_RAG/output/biencoder-nq-dev.questions --gold_data_path Original_RAG/output/biencoder-nq-dev.pages --predictions_path Original_RAG/output/retrieval_preds.tsv --eval_mode retrieval --n_docs 1000 --k 1

원래 score도 다시 계산한다.  
|k|score|  
|---|---|  
|1|68.80|  
|2|40.58|  
|3|29.85|  
|4|23.98|  
|5|20.11|  
|7|15.43|  
|10|11.59|  
|49|3.28|  
|50|3.23|  
|100|1.86|  
|500|0.50|  
|1000|0.28|  




[맨 위로 이동하기](#){: .btn .btn--primary }{: .align-right}