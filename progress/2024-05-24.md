---
title:  "5월 24일 연구 진행 상황" 

categories:
  - Conformal_RAG
tags:
  - [RAG, Conformal_Prediction, Research]

toc: true
toc_sticky: true

date: 2024-05-24
last_modified_at: 2024-05-25
---  

## 1. finetune_rag.py 분석하기  
2024-05-20.md에서 보았듯이 finetune_rag.py를 실행하면 모델이 finetune되지 않고 종료된다. bash 코드는 다음과 같았다.  
```bash
python examples/research_projects/Original_RAG/finetune_rag.py \
    --data_dir examples/research_projects/Original_RAG/finetune_data \
    --output_dir examples/research_projects/Original_RAG/result_folder \
    --model_name_or_path facebook/rag-sequence-base \
    --model_type rag_sequence \
    --fp16 \
    --gpus 1 \
    --distributed_retriever pytorch
```
지금은 data_folder에 다른 data가 많아서 새로 폴더를 만들었다. 폴더 이름은 finetune_data다.  

finetune_data는 계속해서 오류가 발생한다.. 이유는 모르겠다. 
```bash
python finetune_rag.py \
    --data_dir finetune_data \
    --output_dir result_folder \
    --model_name_or_path facebook/rag-sequence-base \
    --model_type rag_sequence \
    --fp16 \
    --gpus 1 \
    --distributed_retriever pytorch \
```

```bash
python finetune_rag.py \
    --data_dir finetune_data \
    --output_dir result_folder \
    --model_name_or_path facebook/rag-sequence-base \
    --model_type rag_sequence \
    --fp16 \
    --gpus 2 \
    --profile \
    --do_train \
    --do_predict \
    --n_val -1 \
    --train_batch_size 8 \
    --eval_batch_size 1 \
    --max_source_length 128 \
    --max_target_length 25 \
    --val_max_target_length 25 \
    --test_max_target_length 25 \
    --label_smoothing 0.1 \
    --dropout 0.1 \
    --attention_dropout 0.1 \
    --weight_decay 0.001 \
    --adam_epsilon 1e-08 \
    --max_grad_norm 0.1 \
    --lr_scheduler polynomial \
    --learning_rate 3e-05 \
    --num_train_epochs 100 \
    --warmup_steps 500 \
    --gradient_accumulation_steps 1 \
    --distributed-port 12345
```
이렇게 코드를 돌렸더니 다음과 같은 오류가 발생한다.  
```bash
INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 2 nodes.
/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:608: UserWarning: Checkpoint directory result_folder exists and is not empty.
  rank_zero_warn(f"Checkpoint directory {dirpath} exists and is not empty.")
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/transformers/optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/transformers/optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
Traceback (most recent call last):
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 722, in _call_and_handle_interrupt
    return self.strategy.launcher.launch(trainer_fn, *args, trainer=self, **kwargs)
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/pytorch_lightning/strategies/launchers/subprocess_script.py", line 93, in launch
    return function(*args, **kwargs)
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 812, in _fit_impl
    results = self._run(model, ckpt_path=self.ckpt_path)
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1237, in _run
    results = self._run_stage()
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1324, in _run_stage
    return self._run_train()
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1346, in _run_train
    self._run_sanity_check()
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1404, in _run_sanity_check
    self._call_callback_hooks("on_sanity_check_start")
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1637, in _call_callback_hooks
    fn(self, self.lightning_module, *args, **kwargs)
  File "/home/flash/Conformal_RAG/transformers/examples/research_projects/Original_RAG/lightning_base.py", line 275, in on_sanity_check_start
    pl_module.model.rag.retriever.init_retrieval()  # better to use hook functions.
TypeError: RagPyTorchDistributedRetriever.init_retrieval() missing 1 required positional argument: 'distributed_port'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/flash/Conformal_RAG/transformers/examples/research_projects/Original_RAG/finetune_rag.py", line 652, in <module>
    main(args)
  File "/home/flash/Conformal_RAG/transformers/examples/research_projects/Original_RAG/finetune_rag.py", line 616, in main
    trainer: pl.Trainer = generic_train(
  File "/home/flash/Conformal_RAG/transformers/examples/research_projects/Original_RAG/lightning_base.py", line 402, in generic_train
    trainer.fit(model)
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 771, in fit
    self._call_and_handle_interrupt(
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 737, in _call_and_handle_interrupt
    self.strategy.reconciliate_processes(traceback.format_exc())
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/pytorch_lightning/strategies/ddp.py", line 446, in reconciliate_processes
    raise DeadlockDetectedException(f"DeadLock detected from rank: {self.global_rank} \n {trace}")
pytorch_lightning.utilities.exceptions.DeadlockDetectedException: DeadLock detected from rank: 0 
 Traceback (most recent call last):
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 722, in _call_and_handle_interrupt
    return self.strategy.launcher.launch(trainer_fn, *args, trainer=self, **kwargs)
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/pytorch_lightning/strategies/launchers/subprocess_script.py", line 93, in launch
    return function(*args, **kwargs)
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 812, in _fit_impl
    results = self._run(model, ckpt_path=self.ckpt_path)
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1237, in _run
    results = self._run_stage()
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1324, in _run_stage
    return self._run_train()
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1346, in _run_train
    self._run_sanity_check()
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1404, in _run_sanity_check
    self._call_callback_hooks("on_sanity_check_start")
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1637, in _call_callback_hooks
    fn(self, self.lightning_module, *args, **kwargs)
  File "/home/flash/Conformal_RAG/transformers/examples/research_projects/Original_RAG/lightning_base.py", line 275, in on_sanity_check_start
    pl_module.model.rag.retriever.init_retrieval()  # better to use hook functions.
TypeError: RagPyTorchDistributedRetriever.init_retrieval() missing 1 required positional argument: 'distributed_port'
```

```bash
ray start --head

python finetune_rag.py \
    --data_dir finetune_data \
    --output_dir result_folder \
    --model_name_or_path facebook/rag-sequence-base \
    --model_type rag_sequence \
    --fp16 \
    --gpus 2 \
    --profile \
    --do_train \
    --do_predict \
    --n_val -1 \
    --train_batch_size 8 \
    --eval_batch_size 1 \
    --max_source_length 128 \
    --max_target_length 25 \
    --val_max_target_length 25 \
    --test_max_target_length 25 \
    --label_smoothing 0.1 \
    --dropout 0.1 \
    --attention_dropout 0.1 \
    --weight_decay 0.001 \
    --adam_epsilon 1e-08 \
    --max_grad_norm 0.1 \
    --lr_scheduler polynomial \
    --learning_rate 3e-05 \
    --num_train_epochs 100 \
    --warmup_steps 500 \
    --gradient_accumulation_steps 1 \
    --distributed_retriever ray \
    --num_retrieval_workers 4
```
이렇게 터미널에 입력했다. distributed_retriever가 default로는 pytorch인데, 이게 문제인가 싶어 ray로 바꾸어보았다.  
돌아가는 듯하더니 다음과 같은 오류와 함께 종료되었다.  
```bash
Traceback (most recent call last):
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 722, in _call_and_handle_interrupt
    return self.strategy.launcher.launch(trainer_fn, *args, trainer=self, **kwargs)
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/pytorch_lightning/strategies/launchers/subprocess_script.py", line 93, in launch
    return function(*args, **kwargs)
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 812, in _fit_impl
    results = self._run(model, ckpt_path=self.ckpt_path)
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1237, in _run
    results = self._run_stage()
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1324, in _run_stage
    return self._run_train()
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1346, in _run_train
    self._run_sanity_check()
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1404, in _run_sanity_check
    self._call_callback_hooks("on_sanity_check_start")
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1637, in _call_callback_hooks
    fn(self, self.lightning_module, *args, **kwargs)
  File "/home/flash/Conformal_RAG/transformers/examples/research_projects/Original_RAG/lightning_base.py", line 275, in on_sanity_check_start
    pl_module.model.rag.retriever.init_retrieval()  # better to use hook functions.
  File "/home/flash/Conformal_RAG/transformers/examples/research_projects/Original_RAG/distributed_ray_retriever.py", line 98, in init_retrieval
    ray.get([worker.init_retrieval.remote() for worker in self.retrieval_workers])
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/ray/_private/worker.py", line 2623, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/ray/_private/worker.py", line 863, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.

Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/flash/Conformal_RAG/transformers/examples/research_projects/Original_RAG/finetune_rag.py", line 652, in <module>
    main(args)
  File "/home/flash/Conformal_RAG/transformers/examples/research_projects/Original_RAG/finetune_rag.py", line 616, in main
    trainer: pl.Trainer = generic_train(
  File "/home/flash/Conformal_RAG/transformers/examples/research_projects/Original_RAG/lightning_base.py", line 402, in generic_train
    trainer.fit(model)
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 771, in fit
    self._call_and_handle_interrupt(
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 737, in _call_and_handle_interrupt
    self.strategy.reconciliate_processes(traceback.format_exc())
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/pytorch_lightning/strategies/ddp.py", line 446, in reconciliate_processes
    raise DeadlockDetectedException(f"DeadLock detected from rank: {self.global_rank} \n {trace}")
pytorch_lightning.utilities.exceptions.DeadlockDetectedException: DeadLock detected from rank: 0 
 Traceback (most recent call last):
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 722, in _call_and_handle_interrupt
    return self.strategy.launcher.launch(trainer_fn, *args, trainer=self, **kwargs)
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/pytorch_lightning/strategies/launchers/subprocess_script.py", line 93, in launch
    return function(*args, **kwargs)
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 812, in _fit_impl
    results = self._run(model, ckpt_path=self.ckpt_path)
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1237, in _run
    results = self._run_stage()
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1324, in _run_stage
    return self._run_train()
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1346, in _run_train
    self._run_sanity_check()
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1404, in _run_sanity_check
    self._call_callback_hooks("on_sanity_check_start")
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1637, in _call_callback_hooks
    fn(self, self.lightning_module, *args, **kwargs)
  File "/home/flash/Conformal_RAG/transformers/examples/research_projects/Original_RAG/lightning_base.py", line 275, in on_sanity_check_start
    pl_module.model.rag.retriever.init_retrieval()  # better to use hook functions.
  File "/home/flash/Conformal_RAG/transformers/examples/research_projects/Original_RAG/distributed_ray_retriever.py", line 98, in init_retrieval
    ray.get([worker.init_retrieval.remote() for worker in self.retrieval_workers])
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/ray/_private/auto_init_hook.py", line 21, in auto_init_wrapper
    return fn(*args, **kwargs)
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/ray/_private/client_mode_hook.py", line 103, in wrapper
    return func(*args, **kwargs)
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/ray/_private/worker.py", line 2623, in get
    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/ray/_private/worker.py", line 863, in get_objects
    raise value
ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.

Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.

(RayRetriever pid=20342) /home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/datasets/load.py:1486: FutureWarning: The repository for wiki_dpr contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/wiki_dpr [repeated 3x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)
(RayRetriever pid=20342) You can avoid this message in future by passing the argument `trust_remote_code=True`. [repeated 3x across cluster]
(RayRetriever pid=20342) Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`. [repeated 3x across cluster]
(RayRetriever pid=20342)   warnings.warn( [repeated 3x across cluster]
```
무슨 ray 용량이 부족하댄다. 일단 원인을 찾아보면, ray에 할당된 용량이 부족한 듯하다. 이것을 확장할 수 있는 방법을 찾아야 한다.  

일단 OOM일 때 kill하는 moniter를 죽이고 실행하도록 다음과 같이 입력을 수정했다.  

```bash
RAY_memory_monitor_refresh_ms=0 ray start --head
```

그랬더니 다음과 같은 오류가 발생한다.  

```bash
Traceback (most recent call last):
  File "/home/flash/Conformal_RAG/transformers/examples/research_projects/Original_RAG/finetune_rag.py", line 652, in <module>
    main(args)
  File "/home/flash/Conformal_RAG/transformers/examples/research_projects/Original_RAG/finetune_rag.py", line 616, in main
    trainer: pl.Trainer = generic_train(
  File "/home/flash/Conformal_RAG/transformers/examples/research_projects/Original_RAG/lightning_base.py", line 402, in generic_train
    trainer.fit(model)
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 771, in fit
    self._call_and_handle_interrupt(
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 724, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 812, in _fit_impl
    results = self._run(model, ckpt_path=self.ckpt_path)
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1237, in _run
    results = self._run_stage()
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1324, in _run_stage
    return self._run_train()
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1354, in _run_train
    self.fit_loop.run()
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py", line 269, in advance
    self._outputs = self.epoch_loop.run(self._data_fetcher)
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py", line 208, in advance
    batch_output = self.batch_loop.run(batch, batch_idx)
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py", line 88, in advance
    outputs = self.optimizer_loop.run(split_batch, optimizers, batch_idx)
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 203, in advance
    result = self._run_optimization(
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 256, in _run_optimization
    self._optimizer_step(optimizer, opt_idx, batch_idx, closure)
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 369, in _optimizer_step
    self.trainer._call_lightning_module_hook(
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1596, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/pytorch_lightning/core/lightning.py", line 1625, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/pytorch_lightning/core/optimizer.py", line 168, in step
    step_output = self._strategy.optimizer_step(self._optimizer, self._optimizer_idx, closure, **kwargs)
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/pytorch_lightning/strategies/ddp.py", line 278, in optimizer_step
    optimizer_output = super().optimizer_step(optimizer, opt_idx, closure, model, **kwargs)
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py", line 193, in optimizer_step
    return self.precision_plugin.optimizer_step(model, optimizer, opt_idx, closure, **kwargs)
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/native_amp.py", line 85, in optimizer_step
    closure_result = closure()
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 148, in __call__
    self._result = self.closure(*args, **kwargs)
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 134, in closure
    step_output = self._step_fn()
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 427, in _training_step
    training_step_output = self.trainer._call_strategy_hook("training_step", *step_kwargs.values())
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1766, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/pytorch_lightning/strategies/ddp.py", line 344, in training_step
    return self.model(*args, **kwargs)
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1040, in forward
    output = self._run_ddp_forward(*inputs, **kwargs)
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1000, in _run_ddp_forward
    return module_to_run(*inputs[0], **kwargs[0])
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/pytorch_lightning/overrides/base.py", line 82, in forward
    output = self.module.training_step(*inputs, **kwargs)
  File "/home/flash/Conformal_RAG/transformers/examples/research_projects/Original_RAG/finetune_rag.py", line 255, in training_step
    loss_tensors = self._step(batch)
  File "/home/flash/Conformal_RAG/transformers/examples/research_projects/Original_RAG/finetune_rag.py", line 238, in _step
    outputs = self(
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/flash/Conformal_RAG/transformers/examples/research_projects/Original_RAG/finetune_rag.py", line 200, in forward
    return self.model(input_ids, **kwargs)
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/transformers/models/rag/modeling_rag.py", line 845, in forward
    outputs = self.rag(
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/transformers/models/rag/modeling_rag.py", line 681, in forward
    gen_outputs = self.generator(
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py", line 1742, in forward
    outputs = self.model(
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py", line 1628, in forward
    decoder_outputs = self.decoder(
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py", line 1481, in forward
    layer_outputs = decoder_layer(
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py", line 767, in forward
    hidden_states, self_attn_weights, present_key_value = self.self_attn(
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py", line 296, in forward
    attn_output = attn_output.reshape(bsz, tgt_len, self.embed_dim)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 1; 23.70 GiB total capacity; 22.49 GiB already allocated; 19.69 MiB free; 22.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "/home/flash/Conformal_RAG/transformers/examples/research_projects/Original_RAG/finetune_rag.py", line 652, in <module>
    main(args)
  File "/home/flash/Conformal_RAG/transformers/examples/research_projects/Original_RAG/finetune_rag.py", line 616, in main
    trainer: pl.Trainer = generic_train(
  File "/home/flash/Conformal_RAG/transformers/examples/research_projects/Original_RAG/lightning_base.py", line 402, in generic_train
    trainer.fit(model)
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 771, in fit
    self._call_and_handle_interrupt(
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 722, in _call_and_handle_interrupt
    return self.strategy.launcher.launch(trainer_fn, *args, trainer=self, **kwargs)
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/pytorch_lightning/strategies/launchers/subprocess_script.py", line 93, in launch
    return function(*args, **kwargs)
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 812, in _fit_impl
    results = self._run(model, ckpt_path=self.ckpt_path)
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1237, in _run
    results = self._run_stage()
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1324, in _run_stage
    return self._run_train()
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1354, in _run_train
    self.fit_loop.run()
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py", line 269, in advance
    self._outputs = self.epoch_loop.run(self._data_fetcher)
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/pytorch_lightning/loops/epoch/training_epoch_loop.py", line 208, in advance
    batch_output = self.batch_loop.run(batch, batch_idx)
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/pytorch_lightning/loops/batch/training_batch_loop.py", line 88, in advance
    outputs = self.optimizer_loop.run(split_batch, optimizers, batch_idx)
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/pytorch_lightning/loops/base.py", line 204, in run
    self.advance(*args, **kwargs)
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 203, in advance
    result = self._run_optimization(
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 256, in _run_optimization
    self._optimizer_step(optimizer, opt_idx, batch_idx, closure)
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 369, in _optimizer_step
    self.trainer._call_lightning_module_hook(
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1596, in _call_lightning_module_hook
    output = fn(*args, **kwargs)
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/pytorch_lightning/core/lightning.py", line 1625, in optimizer_step
    optimizer.step(closure=optimizer_closure)
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/pytorch_lightning/core/optimizer.py", line 168, in step
    step_output = self._strategy.optimizer_step(self._optimizer, self._optimizer_idx, closure, **kwargs)
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/pytorch_lightning/strategies/ddp.py", line 278, in optimizer_step
    optimizer_output = super().optimizer_step(optimizer, opt_idx, closure, model, **kwargs)
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/pytorch_lightning/strategies/strategy.py", line 193, in optimizer_step
    return self.precision_plugin.optimizer_step(model, optimizer, opt_idx, closure, **kwargs)
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/pytorch_lightning/plugins/precision/native_amp.py", line 85, in optimizer_step
    closure_result = closure()
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 148, in __call__
    self._result = self.closure(*args, **kwargs)
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 134, in closure
    step_output = self._step_fn()
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/pytorch_lightning/loops/optimization/optimizer_loop.py", line 427, in _training_step
    training_step_output = self.trainer._call_strategy_hook("training_step", *step_kwargs.values())
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1766, in _call_strategy_hook
    output = fn(*args, **kwargs)
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/pytorch_lightning/strategies/ddp.py", line 344, in training_step
    return self.model(*args, **kwargs)
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1040, in forward
    output = self._run_ddp_forward(*inputs, **kwargs)
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1000, in _run_ddp_forward
    return module_to_run(*inputs[0], **kwargs[0])
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/pytorch_lightning/overrides/base.py", line 82, in forward
    output = self.module.training_step(*inputs, **kwargs)
  File "/home/flash/Conformal_RAG/transformers/examples/research_projects/Original_RAG/finetune_rag.py", line 255, in training_step
    loss_tensors = self._step(batch)
  File "/home/flash/Conformal_RAG/transformers/examples/research_projects/Original_RAG/finetune_rag.py", line 238, in _step
    outputs = self(
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/flash/Conformal_RAG/transformers/examples/research_projects/Original_RAG/finetune_rag.py", line 200, in forward
    return self.model(input_ids, **kwargs)
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/transformers/models/rag/modeling_rag.py", line 845, in forward
    outputs = self.rag(
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/transformers/models/rag/modeling_rag.py", line 681, in forward
    gen_outputs = self.generator(
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py", line 1742, in forward
    outputs = self.model(
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py", line 1628, in forward
    decoder_outputs = self.decoder(
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py", line 1481, in forward
    layer_outputs = decoder_layer(
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py", line 767, in forward
    hidden_states, self_attn_weights, present_key_value = self.self_attn(
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/transformers/models/bart/modeling_bart.py", line 296, in forward
    attn_output = attn_output.reshape(bsz, tgt_len, self.embed_dim)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 23.70 GiB total capacity; 22.50 GiB already allocated; 11.69 MiB free; 22.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
```
이제는 cuda에서 OOM이 발생한다.  
```bash
python finetune_rag.py \
    --data_dir finetune_data \
    --output_dir result_folder \
    --model_name_or_path facebook/rag-sequence-base \
    --model_type rag_sequence \
    --fp16 \
    --gpus 2 \
    --profile \
    --do_train \
    --n_val -1 \
    --train_batch_size 4 \
    --eval_batch_size 1 \
    --max_source_length 128 \
    --max_target_length 25 \
    --val_max_target_length 25 \
    --test_max_target_length 25 \
    --label_smoothing 0.1 \
    --dropout 0.1 \
    --attention_dropout 0.1 \
    --weight_decay 0.001 \
    --adam_epsilon 1e-08 \
    --max_grad_norm 0.1 \
    --lr_scheduler polynomial \
    --learning_rate 3e-05 \
    --num_train_epochs 100 \
    --warmup_steps 500 \
    --gradient_accumulation_steps 1 \
    --distributed_retriever ray \
    --num_retrieval_workers 1
```
일단 batch_size를 낮추어 시도중이다. 제발 되면 좋겠다.  
```bash
Traceback (most recent call last):                                                                                                                                                                                               
  File "/home/flash/Conformal_RAG/transformers/examples/research_projects/Original_RAG/finetune_rag.py", line 652, in <module>
    main(args)
  File "/home/flash/Conformal_RAG/transformers/examples/research_projects/Original_RAG/finetune_rag.py", line 616, in main
    trainer: pl.Trainer = generic_train(
  File "/home/flash/Conformal_RAG/transformers/examples/research_projects/Original_RAG/lightning_base.py", line 402, in generic_train
    trainer.fit(model)
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 771, in fit
    self._call_and_handle_interrupt(
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 724, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 812, in _fit_impl
    results = self._run(model, ckpt_path=self.ckpt_path)
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1237, in _run
    results = self._run_stage()
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1324, in _run_stage
    return self._run_train()
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1354, in _run_train
    self.fit_loop.run()
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/pytorch_lightning/loops/base.py", line 205, in run
    self.on_advance_end()
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py", line 297, in on_advance_end
    self.trainer._call_callback_hooks("on_train_epoch_end")
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1637, in _call_callback_hooks
    fn(self, self.lightning_module, *args, **kwargs)
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py", line 308, in on_train_epoch_end
    self._save_topk_checkpoint(trainer, monitor_candidates)
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py", line 377, in _save_topk_checkpoint
    raise MisconfigurationException(m)
pytorch_lightning.utilities.exceptions.MisconfigurationException: `ModelCheckpoint(monitor='val_em')` could not find the monitored key in the returned metrics: ['epoch', 'step']. HINT: Did you call `log('val_em', value)` in the `LightningModule`?
Traceback (most recent call last):
  File "/home/flash/Conformal_RAG/transformers/examples/research_projects/Original_RAG/finetune_rag.py", line 652, in <module>
    main(args)
  File "/home/flash/Conformal_RAG/transformers/examples/research_projects/Original_RAG/finetune_rag.py", line 616, in main
    trainer: pl.Trainer = generic_train(
  File "/home/flash/Conformal_RAG/transformers/examples/research_projects/Original_RAG/lightning_base.py", line 402, in generic_train
    trainer.fit(model)
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 771, in fit
    self._call_and_handle_interrupt(
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 722, in _call_and_handle_interrupt
    return self.strategy.launcher.launch(trainer_fn, *args, trainer=self, **kwargs)
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/pytorch_lightning/strategies/launchers/subprocess_script.py", line 93, in launch
    return function(*args, **kwargs)
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 812, in _fit_impl
    results = self._run(model, ckpt_path=self.ckpt_path)
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1237, in _run
    results = self._run_stage()
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1324, in _run_stage
    return self._run_train()
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1354, in _run_train
    self.fit_loop.run()
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/pytorch_lightning/loops/base.py", line 205, in run
    self.on_advance_end()
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/pytorch_lightning/loops/fit_loop.py", line 297, in on_advance_end
    self.trainer._call_callback_hooks("on_train_epoch_end")
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py", line 1637, in _call_callback_hooks
    fn(self, self.lightning_module, *args, **kwargs)
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py", line 308, in on_train_epoch_end
    self._save_topk_checkpoint(trainer, monitor_candidates)
  File "/home/flash/Conformal_RAG/.conda/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py", line 377, in _save_topk_checkpoint
    raise MisconfigurationException(m)
pytorch_lightning.utilities.exceptions.MisconfigurationException: `ModelCheckpoint(monitor='val_em')` could not find the monitored key in the returned metrics: ['epoch', 'step']. HINT: Did you call `log('val_em', value)` in the `LightningModule`?
```
이번에는 위와 같은 문제가 발생했다. epoch을 한 번 돌면 바로 이 에러가 뜬다.  

이게 무슨 일인가? lightning_base.py에서 호출되는 여러 함수를 타고 들어가봤는데, lightning.py의 일부 메소드들이 구현되어있지 않다..!  
그런데, 대부분 lightning.py에서 끌어 쓰는 것이 validation set과 test set을 사용하는 과정이다. 결국 과감하게 이 과정을 스킵하도록 코드를 수정하고 다시 학습을 시도했다.  

epoch 100까지 돌아간 이후 코드가 성공적으로 종료된 듯하지만, 모델이 저장되지 않았다.  

다시 코드를 돌리고 있다. 시간이 부족한 관계로 epoch를 많이 줄였고, 코드도 임의로 지우거나 변경한 부분을 원상복구하고 다시 코드를 실행하고 있다.  

```bash
python finetune_rag.py \
    --data_dir finetune_data \
    --output_dir result_folder \
    --model_name_or_path facebook/rag-sequence-base \
    --model_type rag_sequence \
    --fp16 \
    --gpus 2 \
    --profile \
    --do_train \
    --n_val -1 \
    --train_batch_size 4 \
    --eval_batch_size 1 \
    --max_source_length 128 \
    --max_target_length 25 \
    --val_max_target_length 25 \
    --test_max_target_length 25 \
    --label_smoothing 0.1 \
    --dropout 0.1 \
    --attention_dropout 0.1 \
    --weight_decay 0.001 \
    --adam_epsilon 1e-08 \
    --max_grad_norm 0.1 \
    --lr_scheduler polynomial \
    --learning_rate 3e-05 \
    --num_train_epochs 30 \
    --warmup_steps 100 \
    --gradient_accumulation_steps 1 \
    --distributed_retriever ray \
    --num_retrieval_workers 4
```
현재 GPU에 process가 있어서 코드를 돌릴 수 없는 상황이다. 내일 교수님께서 연락을 받으신다면, 진행 방향을 어떻게 해야 할지 결정해야 할 듯하다.  









### 최종 터미널 입력 코드  
```bash
RAY_memory_monitor_refresh_ms=0 ray start --head

python finetune_rag.py \
    --data_dir finetune_data \
    --output_dir result_folder \
    --model_name_or_path facebook/rag-sequence-base \
    --model_type rag_sequence \
    --fp16 \
    --gpus 2 \
    --profile \
    --do_train \
    --n_val -1 \
    --train_batch_size 4 \
    --eval_batch_size 1 \
    --max_source_length 128 \
    --max_target_length 25 \
    --val_max_target_length 25 \
    --test_max_target_length 25 \
    --label_smoothing 0.1 \
    --dropout 0.1 \
    --attention_dropout 0.1 \
    --weight_decay 0.001 \
    --adam_epsilon 1e-08 \
    --max_grad_norm 0.1 \
    --lr_scheduler polynomial \
    --learning_rate 3e-05 \
    --num_train_epochs 30 \
    --warmup_steps 100 \
    --gradient_accumulation_steps 1 \
    --distributed_retriever ray \
    --num_retrieval_workers 4
```

## 2. top-K 찾기  
```bash
python examples/research_projects/Original_RAG/eval_rag.py \
--model_name_or_path facebook/rag-sequence-base \
--model_type rag_sequence \
--evaluation_set examples/research_projects/Original_RAG/data_folder/val_question.txt \
--gold_data_path examples/research_projects/Original_RAG/data_folder/val_answer.txt  \
--predictions_path examples/research_projects/Original_RAG/output/get_k_preds.csv \
--eval_mode retrieval \
--n_docs 200
--find_K
```
지금 이 코드를 돌리고 있는데, 갑자기 다음과 같이 데이터들을 다운로드 받고 있다..!  
```bash
Loading passages from wiki_dpr
https://raw.githubusercontent.com/huggingface/datasets/1.0.1/datasets/wiki_dpr/wiki_dpr.py not found in cache or force_download set to True, downloading to /home/flash/.cache/huggingface/datasets/tmpttho3hud
Downloading: 7.89kB [00:00, 14.4MB/s]                                                                                                                                                                                            
storing https://raw.githubusercontent.com/huggingface/datasets/1.0.1/datasets/wiki_dpr/wiki_dpr.py in cache at /home/flash/.cache/huggingface/datasets/71a49e64f4e26a0781ab57b8182886e610383b33afcfc403d7f12ce704ab3c58.8e129dc9c7a2f2272d074146b9401c674c6a23e07a74cdd46325832b6ce06954.py
creating metadata file for /home/flash/.cache/huggingface/datasets/71a49e64f4e26a0781ab57b8182886e610383b33afcfc403d7f12ce704ab3c58.8e129dc9c7a2f2272d074146b9401c674c6a23e07a74cdd46325832b6ce06954.py
https://raw.githubusercontent.com/huggingface/datasets/1.0.1/datasets/wiki_dpr/dataset_infos.json not found in cache or force_download set to True, downloading to /home/flash/.cache/huggingface/datasets/tmp_wtyxwze
Downloading: 21.9kB [00:00, 72.2MB/s]                                                                                                                                                                                            
storing https://raw.githubusercontent.com/huggingface/datasets/1.0.1/datasets/wiki_dpr/dataset_infos.json in cache at /home/flash/.cache/huggingface/datasets/6e7f03c42bef0bede4731ff37fafe97ba70f29270e652f2c04bd6a94c5b63525.e35afab11b330cfcffe0799a4753e13b401014ffc82e7e6d66538a7f063dc4d2
creating metadata file for /home/flash/.cache/huggingface/datasets/6e7f03c42bef0bede4731ff37fafe97ba70f29270e652f2c04bd6a94c5b63525.e35afab11b330cfcffe0799a4753e13b401014ffc82e7e6d66538a7f063dc4d2
Checking /home/flash/.cache/huggingface/datasets/71a49e64f4e26a0781ab57b8182886e610383b33afcfc403d7f12ce704ab3c58.8e129dc9c7a2f2272d074146b9401c674c6a23e07a74cdd46325832b6ce06954.py for additional imports.
Found main folder for dataset https://raw.githubusercontent.com/huggingface/datasets/1.0.1/datasets/wiki_dpr/wiki_dpr.py at /home/flash/.cache/huggingface/modules/datasets_modules/datasets/wiki_dpr
Creating specific version folder for dataset https://raw.githubusercontent.com/huggingface/datasets/1.0.1/datasets/wiki_dpr/wiki_dpr.py at /home/flash/.cache/huggingface/modules/datasets_modules/datasets/wiki_dpr/ed4af53a9dc3c0075eebc1eaab653f9438509f68690307b728d5cc5741cf041d
Copying script file from https://raw.githubusercontent.com/huggingface/datasets/1.0.1/datasets/wiki_dpr/wiki_dpr.py to /home/flash/.cache/huggingface/modules/datasets_modules/datasets/wiki_dpr/ed4af53a9dc3c0075eebc1eaab653f9438509f68690307b728d5cc5741cf041d/wiki_dpr.py
Copying dataset infos file from https://raw.githubusercontent.com/huggingface/datasets/1.0.1/datasets/wiki_dpr/dataset_infos.json to /home/flash/.cache/huggingface/modules/datasets_modules/datasets/wiki_dpr/ed4af53a9dc3c0075eebc1eaab653f9438509f68690307b728d5cc5741cf041d/dataset_infos.json
Creating metadata file for dataset https://raw.githubusercontent.com/huggingface/datasets/1.0.1/datasets/wiki_dpr/wiki_dpr.py at /home/flash/.cache/huggingface/modules/datasets_modules/datasets/wiki_dpr/ed4af53a9dc3c0075eebc1eaab653f9438509f68690307b728d5cc5741cf041d/wiki_dpr.json
Using custom data configuration psgs_w100.nq.no_index
Loading Dataset Infos from /home/flash/.cache/huggingface/modules/datasets_modules/datasets/wiki_dpr/ed4af53a9dc3c0075eebc1eaab653f9438509f68690307b728d5cc5741cf041d
Generating dataset wiki_dpr (/home/flash/.cache/huggingface/datasets/wiki_dpr/psgs_w100.nq.no_index/0.0.0/ed4af53a9dc3c0075eebc1eaab653f9438509f68690307b728d5cc5741cf041d)
https://storage.googleapis.com/huggingface-nlp/cache/datasets/wiki_dpr/psgs_w100.nq.no_index/0.0.0/dataset_info.json not found in cache or force_download set to True, downloading to /home/flash/.cache/huggingface/datasets/tmpyaiz276d
Downloading: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 11.2k/11.2k [00:00<00:00, 3.62MB/s]
storing https://storage.googleapis.com/huggingface-nlp/cache/datasets/wiki_dpr/psgs_w100.nq.no_index/0.0.0/dataset_info.json in cache at /home/flash/.cache/huggingface/datasets/0bd08ea76ad9c451f5aeaf24c3680717d1b797d69626b7b46a6b5b0ac8796684.4fd8d0523297fdd3cbe46873fd88f08f53309f6f0f75ab49c2bb1d3fa85959dd
creating metadata file for /home/flash/.cache/huggingface/datasets/0bd08ea76ad9c451f5aeaf24c3680717d1b797d69626b7b46a6b5b0ac8796684.4fd8d0523297fdd3cbe46873fd88f08f53309f6f0f75ab49c2bb1d3fa85959dd
Loading Dataset info from /home/flash/.cache/huggingface/datasets/wiki_dpr/psgs_w100.nq.no_index/0.0.0/ed4af53a9dc3c0075eebc1eaab653f9438509f68690307b728d5cc5741cf041d.incomplete
https://storage.googleapis.com/huggingface-nlp/cache/datasets/wiki_dpr/psgs_w100.nq.no_index/0.0.0/wiki_dpr-train.arrow not found in cache or force_download set to True, downloading to /home/flash/.cache/huggingface/datasets/tmp20ji34cl
Downloading: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 78.4G/78.4G [1:07:33<00:00, 19.3MB/s]
storing https://storage.googleapis.com/huggingface-nlp/cache/datasets/wiki_dpr/psgs_w100.nq.no_index/0.0.0/wiki_dpr-train.arrow in cache at /home/flash/.cache/huggingface/datasets/bf9f0c19fc7f1834c589f29f72942bc195b0fc6f31e47573afc18dead4ee26dc.1dc62d618c26ac54a1a24b0a5dbeef50d93ad3e8ebeb4fb0134cb321e9e08247
creating metadata file for /home/flash/.cache/huggingface/datasets/bf9f0c19fc7f1834c589f29f72942bc195b0fc6f31e47573afc18dead4ee26dc.1dc62d618c26ac54a1a24b0a5dbeef50d93ad3e8ebeb4fb0134cb321e9e08247
Loading Dataset info from /home/flash/.cache/huggingface/datasets/wiki_dpr/psgs_w100.nq.no_index/0.0.0/ed4af53a9dc3c0075eebc1eaab653f9438509f68690307b728d5cc5741cf041d.incomplete
Dataset downloaded from Hf google storage.
Constructing Dataset for split train, from /home/flash/.cache/huggingface/datasets/wiki_dpr/psgs_w100.nq.no_index/0.0.0/ed4af53a9dc3c0075eebc1eaab653f9438509f68690307b728d5cc5741cf041d
Unable to verify checksums.
Set __getitem__(key) output type to numpy for ['embeddings'] columns  (when key is int or slice) and do output other (un-formatted) columns.
initializing retrieval
Loading index from wiki_dpr with index name exact
Checking /home/flash/.cache/huggingface/datasets/71a49e64f4e26a0781ab57b8182886e610383b33afcfc403d7f12ce704ab3c58.8e129dc9c7a2f2272d074146b9401c674c6a23e07a74cdd46325832b6ce06954.py for additional imports.
Found main folder for dataset https://raw.githubusercontent.com/huggingface/datasets/1.0.1/datasets/wiki_dpr/wiki_dpr.py at /home/flash/.cache/huggingface/modules/datasets_modules/datasets/wiki_dpr
Found specific version folder for dataset https://raw.githubusercontent.com/huggingface/datasets/1.0.1/datasets/wiki_dpr/wiki_dpr.py at /home/flash/.cache/huggingface/modules/datasets_modules/datasets/wiki_dpr/ed4af53a9dc3c0075eebc1eaab653f9438509f68690307b728d5cc5741cf041d
Found script file from https://raw.githubusercontent.com/huggingface/datasets/1.0.1/datasets/wiki_dpr/wiki_dpr.py to /home/flash/.cache/huggingface/modules/datasets_modules/datasets/wiki_dpr/ed4af53a9dc3c0075eebc1eaab653f9438509f68690307b728d5cc5741cf041d/wiki_dpr.py
Found dataset infos file from https://raw.githubusercontent.com/huggingface/datasets/1.0.1/datasets/wiki_dpr/dataset_infos.json to /home/flash/.cache/huggingface/modules/datasets_modules/datasets/wiki_dpr/ed4af53a9dc3c0075eebc1eaab653f9438509f68690307b728d5cc5741cf041d/dataset_infos.json
Found metadata file for dataset https://raw.githubusercontent.com/huggingface/datasets/1.0.1/datasets/wiki_dpr/wiki_dpr.py at /home/flash/.cache/huggingface/modules/datasets_modules/datasets/wiki_dpr/ed4af53a9dc3c0075eebc1eaab653f9438509f68690307b728d5cc5741cf041d/wiki_dpr.json
Using custom data configuration psgs_w100.nq.exact
Loading Dataset Infos from /home/flash/.cache/huggingface/modules/datasets_modules/datasets/wiki_dpr/ed4af53a9dc3c0075eebc1eaab653f9438509f68690307b728d5cc5741cf041d
Generating dataset wiki_dpr (/home/flash/.cache/huggingface/datasets/wiki_dpr/psgs_w100.nq.exact/0.0.0/ed4af53a9dc3c0075eebc1eaab653f9438509f68690307b728d5cc5741cf041d)
Dataset not on Hf google storage. Downloading and preparing it from source
  0%|                                                                                                                                                                                                      | 0/1 [00:00<?, ?it/s]https://dl.fbaipublicfiles.com/dpr/wikipedia_split/psgs_w100.tsv.gz not found in cache or force_download set to True, downloading to /home/flash/.cache/huggingface/datasets/downloads/tmp315w_79q
Downloading: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4.69G/4.69G [01:04<00:00, 72.4MB/s]
storing https://dl.fbaipublicfiles.com/dpr/wikipedia_split/psgs_w100.tsv.gz in cache at /home/flash/.cache/huggingface/datasets/downloads/affb529aa0c7525a13edd817829e0b9ae1c573ae10b403c3320fcb0119adebc3[01:04<00:00, 96.4MB/s]
creating metadata file for /home/flash/.cache/huggingface/datasets/downloads/affb529aa0c7525a13edd817829e0b9ae1c573ae10b403c3320fcb0119adebc3
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [01:05<00:00, 65.01s/it]
Downloading took 1.0 min
Checksum Computation took 0.0 min
```
아마 train set이 이것인 듯하다. 나중에 future work로 쓰면 좋을 듯하다.  
위치는 잘 모르겠지만, 대충 home/flash/.cache/huggingface로 가면 된다.  
일단 데이터가 전부 다운이 된 이후에는 evaluation이 될 것 같은데, 시간이 꽤 걸릴 듯하다. 갑자기 어떤 파일을 찾을 수 없다며 종료되었다. 아래 코드로 다시 돌려보았다. (cd로 Original_RAG까지 들어왔다.)  

```bash
python eval_rag.py \
--model_name_or_path facebook/rag-sequence-base \
--model_type rag_sequence \
--evaluation_set data_folder/val_question.txt \
--gold_data_path data_folder/val_answer.txt  \
--predictions_path output/get_k_preds.csv \
--eval_mode retrieval \
--n_docs 200 \
--find_K

```
이렇게 하니까 또 코드가 돌아간다. 일단 이유를 분석하고 싶지만, 시간이 없다.  
결과는 다음과 같다.  
```bash
INFO:__main__:minimum k is 34
INFO:__main__:Precision@34:  90.05
```  
나중에 finetuning할 때 k=34를 쓰면 된다.  

## 3. 포스터 제작  
포스터에 들어가야 할 내용을 정리해보자.  
1. 연구의 개요 (접근 방식)  
    1) 기존의 모델에 CP를 적용.  
        - 어떤 방식으로 연구를 진행할지  
    2) 기존의 모델과 비교하는 diagram (progress에 넣었던 diagram 그대로 사용)  

2. 식 정리 (이 부분을 오히려 중점적으로 해야 할 듯 ㅋㅋㅋㅋㅋ)  
    1) 수식이 의미하는 바를 diagram으로 크게  
    2) 아래에 수식 표시  

3. 연구 결과 (없음)  
    1) ㅅㅂ  

4. future work  
    1) 더 큰 database로 finetuning해봐야 함.  
    2) 

### 필요한 diagram  
1. CP로 나타낸 부분에 대한 diagram (Top-K에 관한 diagram)  
2. 기존 LLM 모델과의 비교 (background)  
3. 
써놓고 보니 1번과 2번이 약간 겹친다.  
