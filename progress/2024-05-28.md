---
title:  "5월 26일 연구 진행 상황" 

categories:
  - Conformal_RAG
tags:
  - [RAG, Conformal_Prediction, Research]

toc: true
toc_sticky: true

date: 2024-05-26
last_modified_at: 2024-05-26
---  

> 2024-05-26.md에 내가 똥을 싸서, 새로운 markdown 파일을 생성했다.  

### 1. data 만들기  
먼저 evaluation할 data가 필요하다. 이는 Original_RAG에서 생성했던 데이터를 그대로 가져왔다.  
데이터 수가 너무 많으면 finetuning이 너무 힘들어서, training data의 수를 500개로 제한한다.  


## 2. get_K  
### 1st try  
2024-05-26.md에 있는 코드를 재사용한다.  
```bash
python eval_rag.py \
--model_name_or_path facebook/rag-sequence-base \
--model_type rag_sequence \
--evaluation_set data/val_question.txt \
--gold_data_path data/val_answer.txt  \
--predictions_path output/get_k_preds.csv \
--eval_mode retrieval \
--n_docs 200 \
--recalculate \
--get_K
```
2024-05-26.md에서는 이 코드가 정상적으로 수행되었는데, 지금은 실행되지 않는다.  
다음과 같은 오류와 함께..  
```bash
Traceback (most recent call last):
  File "/home/flash/Conformal_RAG/My_code/rag-end2end-retriever/eval_rag.py", line 355, in <module>
    main(args)
  File "/home/flash/Conformal_RAG/My_code/rag-end2end-retriever/eval_rag.py", line 329, in main
    retriever = RagRetriever.from_pretrained(checkpoint, **model_kwargs)
  File "/home/flash/anaconda3/lib/python3.10/site-packages/transformers/models/rag/retrieval_rag.py", line 452, in from_pretrained
    return cls(
  File "/home/flash/anaconda3/lib/python3.10/site-packages/transformers/models/rag/retrieval_rag.py", line 411, in __init__
    self.init_retrieval()
  File "/home/flash/anaconda3/lib/python3.10/site-packages/transformers/models/rag/retrieval_rag.py", line 485, in init_retrieval
    self.index.init_index()
  File "/home/flash/anaconda3/lib/python3.10/site-packages/transformers/models/rag/retrieval_rag.py", line 295, in init_index
    self.dataset = load_dataset(
  File "/home/flash/anaconda3/lib/python3.10/site-packages/datasets/load.py", line 608, in load_dataset
    builder_instance.download_and_prepare(
  File "/home/flash/anaconda3/lib/python3.10/site-packages/datasets/builder.py", line 404, in download_and_prepare
    self.download_post_processing_resources(dl_manager)
  File "/home/flash/anaconda3/lib/python3.10/site-packages/datasets/builder.py", line 564, in download_post_processing_resources
    downloaded_resource_path = self._download_post_processing_resources(
  File "/home/flash/.cache/huggingface/modules/datasets_modules/datasets/wiki_dpr/ed4af53a9dc3c0075eebc1eaab653f9438509f68690307b728d5cc5741cf041d/wiki_dpr.py", line 154, in _download_post_processing_resources
    downloaded_resources = dl_manager.download_and_extract(
  File "/home/flash/anaconda3/lib/python3.10/site-packages/datasets/utils/download_manager.py", line 251, in download_and_extract
    return self.extract(self.download(url_or_urls))
  File "/home/flash/anaconda3/lib/python3.10/site-packages/datasets/utils/download_manager.py", line 175, in download
    downloaded_path_or_paths = map_nested(
  File "/home/flash/anaconda3/lib/python3.10/site-packages/datasets/utils/py_utils.py", line 224, in map_nested
    mapped = [
  File "/home/flash/anaconda3/lib/python3.10/site-packages/datasets/utils/py_utils.py", line 225, in <listcomp>
    _single_map_nested((function, obj, types, None, True)) for obj in tqdm(iterable, disable=disable_tqdm)
  File "/home/flash/anaconda3/lib/python3.10/site-packages/datasets/utils/py_utils.py", line 163, in _single_map_nested
    return function(data_struct)
  File "/home/flash/anaconda3/lib/python3.10/site-packages/datasets/utils/file_utils.py", line 300, in cached_path
    output_path = get_from_cache(
  File "/home/flash/anaconda3/lib/python3.10/site-packages/datasets/utils/file_utils.py", line 474, in get_from_cache
    raise FileNotFoundError("Couldn't find file at {}".format(url))
FileNotFoundError: Couldn't find file at https://storage.googleapis.com/huggingface-nlp/datasets/wiki_dpr/psgs_w100.nq.IndexHNSWFlat-IP-train.faiss  
```
나중에 재시도해본다.  

다시 돌려보니 정상적으로 돌아간다.  
결과는 k=13이었다.  


### 3. 비교하기  
아무리 찾아보아도, k나 n_docs를 넘길 수 있는 방법은 없었다.  
결국 finetune_rag.py를 직접적으로 수정했다.  
[수정한 부분](My_code/rag-end2end-retriever/finetune_rag.py)  
line 118을 수정했다.  


#### 1) standard model  
standard model은 n_docs가 기본적으로 5로 세팅되어 있다. 이 세팅으로 finetuning한다.  
```bash
RAY_memory_monitor_refresh_ms=0 ray start --head

python use_own_knowledge_dataset.py  \
    --csv_path finetune_data/SQUAD-KB/squad-kb.csv \
    --output_dir finetune_data/SQUAD-KB

python finetune_rag.py \
    --data_dir  finetune_data/squad-training-data \
    --output_dir output/model/standard \
    --model_name_or_path facebook/rag-sequence-base \
    --model_type rag_sequence \
    --fp16 \
    --gpus 2  \
    --profile \
    --do_train \
    --end2end \
    --do_predict \
    --n_val -1  \
    --train_batch_size 4 \
    --eval_batch_size 1 \
    --max_source_length 128 \
    --max_target_length 25 \
    --val_max_target_length 25 \
    --test_max_target_length 25 \
    --weight_decay 0.001 \
    --adam_epsilon 1e-08 \
    --max_grad_norm 0.1 \
    --lr_scheduler polynomial \
    --learning_rate 3e-06 \
    --num_train_epochs 3 \
    --warmup_steps 50 \
    --gradient_accumulation_steps 1 \
    --distributed_retriever ray \
    --num_retrieval_workers 4  \
    --passages_path finetune_data/SQUAD-KB/my_knowledge_dataset \
    --index_path  finetune_data/SQUAD-KB/my_knowledge_dataset_hnsw_index.faiss \
    --index_name custom \
    --context_encoder_name facebook/dpr-ctx_encoder-multiset-base \
    --index_gpus 1 \
    --gpu_order [5,6,7,8,9,0,1,2,3,4] \
    --indexing_freq 5
```

아래 코드로 evaluation한다.  
```bash
python eval_rag.py \
    --model_name_or_path output/model/standard/checkpoint64 \
    --model_type rag_sequence \
    --evaluation_set data/test_question.txt \
    --gold_data_path data/test_gold_data.txt \
    --predictions_path output/standard_e2e_preds.tsv \
    --eval_mode e2e \
    --gold_data_mode qa \
    --n_docs 5 \
    --k 5 \
    --print_predictions \
    --recalculate
```

아주 큰 문제점을 발견했다. wiki_dpr을 사용할 수 없다. custom_index를 사용해야 한다. 즉, wiki를 내가 만들어야 한다는 뜻이다...
그러기 위해서는 test_run/dummy-kb 폴더에 my_knowledge_dataset.csv가 필요하다.  

일단 진행 ㄱㄱ  

```bash
python use_own_knowledge_dataset.py  \
    --csv_path test_run/dummy-kb/my_knowledge_dataset.csv \
    --output_dir  test_run/dummy-kb

python finetune_rag.py \
    --data_dir  finetune_data/squad-training-data \
    --output_dir output/model/standard \
    --model_name_or_path facebook/rag-sequence-base \
    --model_type rag_sequence \
    --fp16 \
    --gpus 2  \
    --profile \
    --do_train \
    --end2end \
    --do_predict \
    --n_val -1  \
    --train_batch_size 4 \
    --eval_batch_size 1 \
    --max_source_length 128 \
    --max_target_length 25 \
    --val_max_target_length 25 \
    --test_max_target_length 25 \
    --weight_decay 0.001 \
    --adam_epsilon 1e-08 \
    --max_grad_norm 0.1 \
    --lr_scheduler polynomial \
    --learning_rate 3e-06 \
    --num_train_epochs 3 \
    --warmup_steps 50 \
    --gradient_accumulation_steps 1 \
    --distributed_retriever ray \
    --num_retrieval_workers 4  \
    --index_name custom \
    --context_encoder_name facebook/dpr-ctx_encoder-multiset-base \
    --index_gpus 1 \
    --gpu_order [5,6,7,8,9,0,1,2,3,4] \
    --indexing_freq 5
```

```bash
python eval_rag.py \
    --model_name_or_path output/model/standard/checkpoint12 \
    --model_type rag_sequence \
    --evaluation_set test_run/dummy-train-data/test.source \
    --gold_data_path test_run/dummy-train-data/test.target \
    --predictions_path output/standard_e2e_preds.tsv \
    --eval_mode e2e \
    --gold_data_mode qa \
    --n_docs 5 \
    --k 5 \
    --print_predictions \
    --recalculate
```

일단 학습 자체는 되는 것 같다. 문제는 dataset을 어떻게 만드는지 모르겠다. 기본적으로 제공된 dataset에 Plato와 Socrates wiki를 덮었고, 나머지는 그냥 typing해서 추가했는데, Plato와 Socrates만 학습한 것 같다.  

이후부터는 다시 쓰겠다. get_K부터 다시 써야 된다.  