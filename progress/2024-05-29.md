## 1. SQuAD 데이터 가공  
안타깝게도, 어제 한 일이 모두 뻘짓이었다.  
[SQuAD 데이터 다운로드](My_code/My_RAG/data/SQuAD.ipynb)  
위의 코드를 따라 SQuAD 데이터를 가공한다.  

이후 아래 코드를 입력해 finetuning을 하자.  
```bash
python use_own_knowledge_dataset.py --csv_path finetune_data/t_squad-kb.csv --output_dir finetune_data/SQUAD-KB

RAY_memory_monitor_refresh_ms=0 ray start --head

python finetune_rag.py \
    --data_dir  finetune_data/squad-training-data/set1 \
    --output_dir output/model/standard/1st \
    --model_name_or_path facebook/rag-token-base \
    --model_type rag_token \
    --fp16 \
    --gpus 2  \
    --profile \
    --do_train \
    --end2end \
    --do_predict \
    --n_val -1  \
    --train_batch_size 4 \
    --eval_batch_size 1 \
    --max_source_length 128 \
    --max_target_length 25 \
    --val_max_target_length 25 \
    --test_max_target_length 25 \
    --label_smoothing 0.1 \
    --dropout 0.1 \
    --attention_dropout 0.1 \
    --weight_decay 0.001 \
    --adam_epsilon 1e-08 \
    --max_grad_norm 0.1 \
    --lr_scheduler polynomial \
    --learning_rate 3e-05 \
    --num_train_epochs 5 \
    --warmup_steps 500 \
    --gradient_accumulation_steps 8 \
    --distributed_retriever ray \
    --num_retrieval_workers 4  \
    --passages_path finetune_data/SQUAD-KB/my_knowledge_dataset \
    --index_path  finetune_data/SQUAD-KB/my_knowledge_dataset_hnsw_index.faiss \
    --index_name custom \
    --context_encoder_name facebook/dpr-ctx_encoder-multiset-base \
    --index_gpus 1 \
    --gpu_order [5,6,7,8,9,0,1,2,3,4] \
    --indexing_freq 500
```

```bash
python eval_rag.py \
    --model_name_or_path output/model/standard/1st/checkpoint1251 \
    --model_type rag_token \
    --evaluation_set finetune_data/squad-training-data/set1/test.source \
    --gold_data_path finetune_data/squad-training-data/set1/test.target \
    --predictions_path output/preds/1st_preds.tsv \
    --eval_mode e2e \
    --gold_data_mode ans \
    --n_docs 5 \
    --k 1 \
    --print_predictions \
    --recalculate
```
일단 아래 출력 keep  
```bash
INFO:__main__:F1: 3.78
INFO:__main__:EM: 0.00
```

```bash
python eval_rag.py \
    --model_name_or_path facebook/rag-token-base \
    --model_type rag_token \
    --evaluation_set finetune_data/squad-training-data/original/ext-source \
    --gold_data_path finetune_data/squad-training-data/ext-retrieval \
    --predictions_path output/preds/standard_retrieval_preds.tsv \
    --eval_mode retrieval \
    --n_docs 100 \
    --get_K \
    --error_rate 0.5 \
    --recalculate 
```

설마 use_own_knowledge_dataset.py가 잘 돌아간다 싶더니, 역시 잘못된 건가..?  
진짜 큰일인데  
일단 데이터셋을 다시 추출했다. (n=1000)  
top-K부터 구해보자.  

## 2. Top-K 구하기  
일단 전체 n에 대해 다 구해볼 생각이다.  
```bash
python eval_rag.py \
    --model_name_or_path facebook/rag-token-base \
    --model_type rag_token \
    --evaluation_set finetune_data/squad-training-data/original/ext-source \
    --gold_data_path finetune_data/squad-training-data/original/ext-retrieval \
    --predictions_path output/preds/standard_retrieval_preds.tsv \
    --eval_mode retrieval \
    --n_docs 1000 \
    --get_K \
    --error_rate 0.5 \
    --recalculate 
```
역시 k가 없다.  

일단 생각한 방안은 다음과 같다.  
squad data에서 set1과 set2 두 개 추출  
set1으로 먼저 finetuning  
set2로 finetuning하기 전에, 위에서 finetuning한 모델을 사용해 set2의 validation set으로 top-K를 구하기  
이후 standard와 top-K로 나누어 finetuning후 evaluation.  

### 1. set1으로 finetuning  
```bash
python use_own_knowledge_dataset.py --csv_path finetune_data/t_squad-kb.csv --output_dir finetune_data/SQUAD-KB

RAY_memory_monitor_refresh_ms=0 ray start --head

python finetune_rag.py \
    --data_dir  finetune_data/squad-training-data/set1 \
    --output_dir output/model/standard/1st \
    --model_name_or_path facebook/rag-token-base \
    --model_type rag_token \
    --fp16 \
    --gpus 2  \
    --profile \
    --do_train \
    --end2end \
    --do_predict \
    --n_val -1  \
    --train_batch_size 4 \
    --eval_batch_size 1 \
    --max_source_length 128 \
    --max_target_length 25 \
    --val_max_target_length 25 \
    --test_max_target_length 25 \
    --label_smoothing 0.1 \
    --dropout 0.1 \
    --attention_dropout 0.1 \
    --weight_decay 0.001 \
    --adam_epsilon 1e-08 \
    --max_grad_norm 0.1 \
    --lr_scheduler polynomial \
    --learning_rate 3e-05 \
    --num_train_epochs 5 \
    --warmup_steps 500 \
    --gradient_accumulation_steps 8 \
    --distributed_retriever ray \
    --num_retrieval_workers 4  \
    --passages_path finetune_data/SQUAD-KB/my_knowledge_dataset \
    --index_path  finetune_data/SQUAD-KB/my_knowledge_dataset_hnsw_index.faiss \
    --index_name custom \
    --context_encoder_name facebook/dpr-ctx_encoder-multiset-base \
    --index_gpus 1 \
    --gpu_order [5,6,7,8,9,0,1,2,3,4] \
    --indexing_freq 500
```

### set2로 top-K 구하기  
```bash
python eval_rag.py \
    --model_name_or_path output/model/standard/1st/checkpoint1251 \
    --model_type rag_token \
    --evaluation_set finetune_data/squad-training-data/set2/val.source \
    --gold_data_path finetune_data/squad-training-data/set2/val.retrieval \
    --predictions_path output/preds/1st_retrieval_preds.tsv \
    --eval_mode retrieval \
    --n_docs 1000 \
    --get_K \
    --error_rate 0.5 
```
아쉽게도, error_rate가 0.1일 때에는 k가 없고, 0.45는 있긴 하지만, k가 139다. 적당히 타협해서, k=15로 잡아야겠다.($\alpha$ = 0.5)  

### set2로 standard model 학습하기  
```bash
python finetune_rag.py \
    --data_dir  finetune_data/squad-training-data/set2 \
    --output_dir output/model/2nd/standard \
    --model_name_or_path output/model/1st/checkpoint1251 \
    --model_type rag_token \
    --fp16 \
    --gpus 2  \
    --profile \
    --do_train \
    --end2end \
    --do_predict \
    --n_val -1  \
    --train_batch_size 1 \
    --eval_batch_size 1 \
    --max_source_length 128 \
    --max_target_length 25 \
    --val_max_target_length 25 \
    --test_max_target_length 25 \
    --label_smoothing 0.1 \
    --dropout 0.1 \
    --attention_dropout 0.1 \
    --weight_decay 0.001 \
    --adam_epsilon 1e-08 \
    --max_grad_norm 0.1 \
    --lr_scheduler polynomial \
    --learning_rate 3e-05 \
    --num_train_epochs 3 \
    --warmup_steps 500 \
    --gradient_accumulation_steps 8 \
    --distributed_retriever ray \
    --num_retrieval_workers 4  \
    --passages_path finetune_data/SQUAD-KB/my_knowledge_dataset \
    --index_path  finetune_data/SQUAD-KB/my_knowledge_dataset_hnsw_index.faiss \
    --index_name custom \
    --context_encoder_name facebook/dpr-ctx_encoder-multiset-base \
    --index_gpus 1 \
    --gpu_order [5,6,7,8,9,0,1,2,3,4] \
    --indexing_freq 500
```

### top-k 모델 학습하기  
```bash
python finetune_rag.py \
    --data_dir  finetune_data/squad-training-data/set2 \
    --output_dir output/model/top-k \
    --model_name_or_path output/model/2nd/top-k/checkpoint501 \
    --model_type rag_token \
    --fp16 \
    --gpus 2  \
    --profile \
    --do_train \
    --end2end \
    --do_predict \
    --n_val -1  \
    --train_batch_size 1 \
    --eval_batch_size 1 \
    --max_source_length 128 \
    --max_target_length 25 \
    --val_max_target_length 25 \
    --test_max_target_length 25 \
    --label_smoothing 0.1 \
    --dropout 0.1 \
    --attention_dropout 0.1 \
    --weight_decay 0.001 \
    --adam_epsilon 1e-08 \
    --max_grad_norm 0.1 \
    --lr_scheduler polynomial \
    --learning_rate 3e-05 \
    --num_train_epochs 3 \
    --warmup_steps 500 \
    --gradient_accumulation_steps 8 \
    --distributed_retriever ray \
    --num_retrieval_workers 4  \
    --passages_path finetune_data/SQUAD-KB/my_knowledge_dataset \
    --index_path  finetune_data/SQUAD-KB/my_knowledge_dataset_hnsw_index.faiss \
    --index_name custom \
    --context_encoder_name facebook/dpr-ctx_encoder-multiset-base \
    --index_gpus 1 \
    --gpu_order [5,6,7,8,9,0,1,2,3,4] \
    --indexing_freq 500
```

### evaluation  
```bash
python eval_rag.py \
    --model_name_or_path output/model/2nd/standard/checkpoint316 \
    --model_type rag_token \
    --evaluation_set finetune_data/squad-training-data/set2/test.source \
    --gold_data_path finetune_data/squad-training-data/set2/test.target \
    --predictions_path output/preds/2st_standard_preds.tsv \
    --eval_mode e2e \
    --gold_data_mode ans \
    --n_docs 5 \
    --k 1 \
    --print_predictions \
    --recalculate
```

```bash
python eval_rag.py \
    --model_name_or_path output/model/2nd/top-k/checkpoint751 \
    --model_type rag_token \
    --evaluation_set finetune_data/squad-training-data/set2/test.source \
    --gold_data_path finetune_data/squad-training-data/set2/test.target \
    --predictions_path output/preds/2st_top-k_preds.tsv \
    --eval_mode e2e \
    --gold_data_mode ans \
    --n_docs 5 \
    --print_predictions \
    --recalculate
```
